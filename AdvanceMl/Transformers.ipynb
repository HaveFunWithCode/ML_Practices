{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b29783d6",
   "metadata": {},
   "source": [
    "# Feature Transformation and Scaling"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4636d898",
   "metadata": {},
   "source": [
    "---------------------------------------------------concept----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41c41bc1",
   "metadata": {},
   "source": [
    "Sometimes our datasets have different columns with different value scales. for example, one column's value range is from 0 to 1000 and another column is from 0 to 1! So model do not treats both features(columns) equally! Actually column with larger value can influence more the result of model. we can prevent this problem by feature scaling.\n",
    "there are a lot of scaling method. you should choose based on your data.\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "id": "b29492ab",
   "metadata": {},
   "source": [
    "---------------------------------------------------implementation---------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a54a15",
   "metadata": {},
   "source": [
    "<b>StandardScaler</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41f53f4",
   "metadata": {},
   "source": [
    "the Standard Scaler scales the values in a way the mean would be 0 and the STD would be  1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "185db53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49ef9aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed8bb020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'feature_names', 'DESCR'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f44b85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test=train_test_split(housing.data,housing.target,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b7a09f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   5.9641       44.            4.35714286 ...    2.22857143\n",
      "    33.99       -118.44      ]\n",
      " [   4.5455       25.            5.51132686 ...    2.50809061\n",
      "    34.68       -118.14      ]\n",
      " [   2.2765       30.            4.02164502 ...    3.81962482\n",
      "    34.07       -117.64      ]\n",
      " ...\n",
      " [   2.0096       22.            4.02702703 ...    3.3963964\n",
      "    33.9        -118.26      ]\n",
      " [   2.6875       23.            6.35744681 ...    2.88510638\n",
      "    36.27       -119.25      ]\n",
      " [   7.5696        4.            8.02617801 ...    2.85340314\n",
      "    34.87       -120.45      ]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8730b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean is : [   3.84886232   28.67421096    5.40042744    1.0940336  1426.09759136\n",
      "    3.10512481   35.63013358 -119.56068868]\n",
      "STD is : [   1.89567269   12.55080901    2.30360322    0.46073619 1107.38404741\n",
      "   12.24310365    2.13846495    2.00399271]\n"
     ]
    }
   ],
   "source": [
    "print(\"the mean is : {}\".format(X_train.mean(axis=0)))\n",
    "print(\"STD is : {}\".format(X_train.std(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "472638a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled=scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a4646a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean after scaling is : [-0.  0. -0. -0.  0.  0. -0. -0.]\n",
      "STD after scaling is : [1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"the mean after scaling is : {}\".format(X_train_scaled.mean(axis=0)))\n",
    "print(\"STD after scaling is : {}\".format(X_train_scaled.std(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c0a8a",
   "metadata": {},
   "source": [
    "<b>MinMax Scaler</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24113b4",
   "metadata": {},
   "source": [
    "It simply scale data between zero to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f21dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "m_scaler = MinMaxScaler()\n",
    "m_scaler.fit(X_train)\n",
    "X_train_m_scaled=m_scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f82a5835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean after scaling is : [0.23095973 0.54263159 0.03458403 0.0225504  0.04982311 0.00189535\n",
      " 0.32767379 0.47702304]\n",
      "STD after scaling is : [0.13073424 0.24609429 0.017493   0.01365819 0.03876988 0.00985294\n",
      " 0.22749627 0.19960087]\n"
     ]
    }
   ],
   "source": [
    "print(\"the mean after scaling is : {}\".format(X_train_m_scaled.mean(axis=0)))\n",
    "print(\"STD after scaling is : {}\".format(X_train_m_scaled.std(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24ae8d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_m_scaled.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f7f50da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000004"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_m_scaled.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f79e6d",
   "metadata": {},
   "source": [
    "<b>Robust Scaler</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d43de1f",
   "metadata": {},
   "source": [
    "Standard Scaler and MinMax Scaler use values like the mean, maximum and minimum values of the columns,which are sensitive to outliers.If our data has many outliers,they absoulutlly affect min,max or mean.So using two above method wouldn't garrantee a balanced data and a normal distribution after scaling. Robust Scaler is another method which is sensitive to outiers. This Mehod first removes the median from our data, then scales the data by the InterQuartile Range(IQR).\n",
    "IRQ is the difference between the first and third quartile of the variable:\n",
    "                                                \n",
    "                                                IQR = Q3 – Q1\n",
    "and the scaled value is :\n",
    "                                        x_scaled = (x – Q1)/(Q3 – Q1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bd694a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "r_scaler = RobustScaler()\n",
    "r_scaler.fit(X_train)\n",
    "X_train_r_scaled=r_scaler.transform(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31711ea7",
   "metadata": {},
   "source": [
    "<b>comparison</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3006d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
